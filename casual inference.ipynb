{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>treatment</th>\n",
       "      <th>y_factual</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>...</th>\n",
       "      <th>x16</th>\n",
       "      <th>x17</th>\n",
       "      <th>x18</th>\n",
       "      <th>x19</th>\n",
       "      <th>x20</th>\n",
       "      <th>x21</th>\n",
       "      <th>x22</th>\n",
       "      <th>x23</th>\n",
       "      <th>x24</th>\n",
       "      <th>x25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>5.599916</td>\n",
       "      <td>-0.528603</td>\n",
       "      <td>-0.343455</td>\n",
       "      <td>1.128554</td>\n",
       "      <td>0.161703</td>\n",
       "      <td>-0.316603</td>\n",
       "      <td>1.295216</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>6.875856</td>\n",
       "      <td>-1.736945</td>\n",
       "      <td>-1.802002</td>\n",
       "      <td>0.383828</td>\n",
       "      <td>2.244320</td>\n",
       "      <td>-0.629189</td>\n",
       "      <td>1.295216</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>2.996273</td>\n",
       "      <td>-0.807451</td>\n",
       "      <td>-0.202946</td>\n",
       "      <td>-0.360898</td>\n",
       "      <td>-0.879606</td>\n",
       "      <td>0.808706</td>\n",
       "      <td>-0.526556</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>1.366206</td>\n",
       "      <td>0.390083</td>\n",
       "      <td>0.596582</td>\n",
       "      <td>-1.850350</td>\n",
       "      <td>-0.879606</td>\n",
       "      <td>-0.004017</td>\n",
       "      <td>-0.857787</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>1.963538</td>\n",
       "      <td>-1.045229</td>\n",
       "      <td>-0.602710</td>\n",
       "      <td>0.011465</td>\n",
       "      <td>0.161703</td>\n",
       "      <td>0.683672</td>\n",
       "      <td>-0.360940</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   treatment  y_factual        x1        x2        x3        x4        x5  \\\n",
       "0       True   5.599916 -0.528603 -0.343455  1.128554  0.161703 -0.316603   \n",
       "1      False   6.875856 -1.736945 -1.802002  0.383828  2.244320 -0.629189   \n",
       "2      False   2.996273 -0.807451 -0.202946 -0.360898 -0.879606  0.808706   \n",
       "3      False   1.366206  0.390083  0.596582 -1.850350 -0.879606 -0.004017   \n",
       "4      False   1.963538 -1.045229 -0.602710  0.011465  0.161703  0.683672   \n",
       "\n",
       "         x6  x7  x8  ...  x16  x17  x18  x19  x20  x21  x22  x23  x24  x25  \n",
       "0  1.295216   1   0  ...    1    1    1    1    0    0    0    0    0    0  \n",
       "1  1.295216   0   0  ...    1    1    1    1    0    0    0    0    0    0  \n",
       "2 -0.526556   0   0  ...    1    0    1    1    0    0    0    0    0    0  \n",
       "3 -0.857787   0   0  ...    1    0    1    1    0    0    0    0    0    0  \n",
       "4 -0.360940   1   0  ...    1    1    1    1    0    0    0    0    0    0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"/Users/lijiazheng/Desktop/ihdp_data.xlsx\")\n",
    "df = df.drop(['y_cfactual','mu0','mu1'], axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "treatment\n",
       "False    608\n",
       "True     139\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['treatment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(df, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Propensity Score Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate propensity scores\n",
    "model = LogisticRegression()\n",
    "model.fit(train_data.drop(['treatment', 'y_factual'], axis=1), train_data['treatment'])\n",
    "train_data['propensity_score'] = model.predict_proba(train_data.drop(['treatment', 'y_factual'], axis=1))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "treated = train_data[train_data['treatment'] == 1]\n",
    "untreated = train_data[train_data['treatment'] == 0]\n",
    "indices, _ = pairwise_distances_argmin_min(treated[['propensity_score']], untreated[['propensity_score']])\n",
    "matched = treated.copy()\n",
    "matched['matched_outcome'] = untreated.iloc[indices]['y_factual'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.972023119649341"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_psm = (matched['y_factual'] - matched['matched_outcome']).mean()\n",
    "att_psm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverse Probability of Treatment Weighting Using the Propensity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop(['treatment', 'y_factual'], axis=1)\n",
    "T_train = train_data['treatment']\n",
    "Y_train = train_data['y_factual']\n",
    "\n",
    "X_test = test_data.drop(['treatment', 'y_factual'], axis=1)\n",
    "T_test = test_data['treatment']\n",
    "Y_test = test_data['y_factual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate propensity scores\n",
    "ps_model = LogisticRegression()\n",
    "ps_model.fit(X_train, T_train)\n",
    "propensity_scores_train = ps_model.predict_proba(X_train)[:, 1]\n",
    "propensity_scores_test = ps_model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IPTW weights\n",
    "weights_train = T_train / propensity_scores_train + (1 - T_train) / (1 - propensity_scores_train)\n",
    "weights_test = T_test / propensity_scores_test + (1 - T_test) / (1 - propensity_scores_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated ATE using IPTW: 3.9455353117075003\n"
     ]
    }
   ],
   "source": [
    "def weighted_least_squares(X, T, Y, weights):\n",
    "    X_with_intercept = np.c_[np.ones(X.shape[0]), X]\n",
    "    WLS_model = np.linalg.lstsq(X_with_intercept * weights[:, np.newaxis], Y * weights, rcond=None)[0]\n",
    "    return WLS_model[1]\n",
    "\n",
    "# Calculate the weighted mean outcome for treated and untreated subjects\n",
    "treated_weights_train = weights_train[T_train == 1]\n",
    "untreated_weights_train = weights_train[T_train == 0]\n",
    "\n",
    "treated_outcomes_train = Y_train[T_train == 1]\n",
    "untreated_outcomes_train = Y_train[T_train == 0]\n",
    "\n",
    "weighted_mean_treated = np.sum(treated_weights_train * treated_outcomes_train) / np.sum(treated_weights_train)\n",
    "weighted_mean_untreated = np.sum(untreated_weights_train * untreated_outcomes_train) / np.sum(untreated_weights_train)\n",
    "\n",
    "# Estimate ATE\n",
    "ate_iptw = weighted_mean_treated - weighted_mean_untreated\n",
    "\n",
    "print(f'Estimated ATE using IPTW: {ate_iptw}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated ATT using DML: [3.90723326]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lijiazheng/Library/Python/3.9/lib/python/site-packages/doubleml/_utils_checks.py:204: UserWarning: Propensity predictions from learner RandomForestClassifier(max_depth=10, random_state=1) for ml_m are close to zero or one (eps=1e-12).\n",
      "  warnings.warn(f'Propensity predictions from learner {str(learner)} for'\n"
     ]
    }
   ],
   "source": [
    "from doubleml import DoubleMLData, DoubleMLPLR\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size=0.3, random_state=1)\n",
    "\n",
    "# Define covariates, treatment, and outcome\n",
    "X = train_data.drop(['treatment', 'y_factual'], axis=1)\n",
    "T = train_data['treatment']\n",
    "Y = train_data['y_factual']\n",
    "\n",
    "# Create DoubleMLData object\n",
    "dml_data = DoubleMLData.from_arrays(X.values, Y.values, T.values)\n",
    "\n",
    "# Define the DML model using RandomForest for both nuisance models\n",
    "ml_g = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=1)\n",
    "ml_m = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=1)\n",
    "\n",
    "dml_plr = DoubleMLPLR(dml_data, ml_g, ml_m, n_folds=5)\n",
    "\n",
    "# Fit the model\n",
    "dml_plr.fit()\n",
    "\n",
    "# Estimate the treatment effect\n",
    "att_dml_1 = dml_plr.coef\n",
    "print(f'Estimated ATT using DML: {att_dml_1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSM Mean Estimate: 3.8519239567903627\n",
      "PSM Variance Estimate: 0.0815964630959717\n",
      "PSM 95% Confidence Interval: [3.189316577515217, 4.342124608303891]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "n_bootstraps = 100\n",
    "bootstrap_estimates_dml = []\n",
    "bootstrap_estimates_psm = []\n",
    "\n",
    "for _ in range(n_bootstraps):\n",
    "    # Resample the data with replacement\n",
    "    X_resampled, Y_resampled, T_resampled = resample(X.values, Y.values, T.values, random_state=_)\n",
    "\n",
    "    ps_model = LogisticRegression()\n",
    "    ps_model.fit(X_resampled, T_resampled)\n",
    "    propensity_scores = ps_model.predict_proba(X_resampled)[:, 1]\n",
    "    \n",
    "    # Perform matching\n",
    "    treated = np.where(T_resampled == 1)[0]\n",
    "    untreated = np.where(T_resampled == 0)[0]\n",
    "    indices, _ = pairwise_distances_argmin_min(propensity_scores[treated].reshape(-1, 1), propensity_scores[untreated].reshape(-1, 1))\n",
    "    \n",
    "    matched_outcomes = Y_resampled[untreated][indices]\n",
    "    att_psm = (Y_resampled[treated] - matched_outcomes).mean()\n",
    "    bootstrap_estimates_psm.append(att_psm)\n",
    "\n",
    "# Convert bootstrap estimates to numpy arrays\n",
    "bootstrap_estimates_psm = np.array(bootstrap_estimates_psm)\n",
    "\n",
    "mean_estimate_psm = np.mean(bootstrap_estimates_psm)\n",
    "variance_estimate_psm = np.var(bootstrap_estimates_psm)\n",
    "ci_lower_psm = np.percentile(bootstrap_estimates_psm, 2.5)\n",
    "ci_upper_psm = np.percentile(bootstrap_estimates_psm, 97.5)\n",
    "\n",
    "print(f'PSM Mean Estimate: {mean_estimate_psm}')\n",
    "print(f'PSM Variance Estimate: {variance_estimate_psm}')\n",
    "print(f'PSM 95% Confidence Interval: [{ci_lower_psm}, {ci_upper_psm}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated ATT using DML with Random Forest: [3.84972142]\n",
      "Estimated ATT using DML with XGBoost: [3.50029822]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lijiazheng/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/lijiazheng/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/lijiazheng/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/lijiazheng/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/lijiazheng/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/lijiazheng/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/lijiazheng/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/lijiazheng/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/lijiazheng/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated ATT using DML with Neural Network: [3.82749805]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lijiazheng/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from doubleml import DoubleMLData, DoubleMLPLR\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "\n",
    "# Create DoubleMLData object\n",
    "dml_data = DoubleMLData.from_arrays(X.values, Y.values, T.values)\n",
    "\n",
    "# Define the DML model using different ML algorithms\n",
    "# Random Forest\n",
    "ml_g_rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=1)\n",
    "ml_m_rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=1)\n",
    "\n",
    "# Gradient Boosting (XGBoost)\n",
    "ml_g_xgb = XGBRegressor(n_estimators=100, max_depth=10, random_state=1)\n",
    "ml_m_xgb = XGBClassifier(n_estimators=100, max_depth=10, random_state=1)\n",
    "\n",
    "# Neural Network\n",
    "ml_g_nn = MLPRegressor(hidden_layer_sizes=(100,), max_iter=500, random_state=1)\n",
    "ml_m_nn = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=1)\n",
    "\n",
    "# Initialize DML models\n",
    "dml_plr_rf = DoubleMLPLR(dml_data, ml_g_rf, ml_m_rf, n_folds=5)\n",
    "dml_plr_xgb = DoubleMLPLR(dml_data, ml_g_xgb, ml_m_xgb, n_folds=5)\n",
    "dml_plr_nn = DoubleMLPLR(dml_data, ml_g_nn, ml_m_nn, n_folds=5)\n",
    "\n",
    "# Fit the models and estimate treatment effects\n",
    "dml_plr_rf.fit()\n",
    "treatment_effect_rf = dml_plr_rf.coef\n",
    "print(f'Estimated ATT using DML with Random Forest: {treatment_effect_rf}')\n",
    "\n",
    "dml_plr_xgb.fit()\n",
    "treatment_effect_xgb = dml_plr_xgb.coef\n",
    "print(f'Estimated ATT using DML with XGBoost: {treatment_effect_xgb}')\n",
    "\n",
    "dml_plr_nn.fit()\n",
    "treatment_effect_nn = dml_plr_nn.coef\n",
    "print(f'Estimated ATT using DML with Neural Network: {treatment_effect_nn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Number of bootstrap samples\n",
    "n_bootstraps = 100\n",
    "bootstrap_estimates_rf = []\n",
    "bootstrap_estimates_xgb = []\n",
    "bootstrap_estimates_nn = []\n",
    "bootstrap_estimates_psm = []\n",
    "\n",
    "for _ in range(n_bootstraps):\n",
    "    # Resample the data with replacement\n",
    "    X_resampled, Y_resampled, T_resampled = resample(X.values, Y.values, T.values, random_state=_)\n",
    "    \n",
    "    # For DML with Random Forest\n",
    "    dml_data_resampled = DoubleMLData.from_arrays(X_resampled, Y_resampled, T_resampled)\n",
    "    dml_plr_resampled_rf = DoubleMLPLR(dml_data_resampled, ml_g_rf, ml_m_rf, n_folds=5)\n",
    "    dml_plr_resampled_rf.fit()\n",
    "    bootstrap_estimates_rf.append(dml_plr_resampled_rf.coef[0])\n",
    "    \n",
    "    # For DML with XGBoost\n",
    "    dml_plr_resampled_xgb = DoubleMLPLR(dml_data_resampled, ml_g_xgb, ml_m_xgb, n_folds=5)\n",
    "    dml_plr_resampled_xgb.fit()\n",
    "    bootstrap_estimates_xgb.append(dml_plr_resampled_xgb.coef[0])\n",
    "    \n",
    "    # For DML with Neural Network\n",
    "    dml_plr_resampled_nn = DoubleMLPLR(dml_data_resampled, ml_g_nn, ml_m_nn, n_folds=5)\n",
    "    dml_plr_resampled_nn.fit()\n",
    "    bootstrap_estimates_nn.append(dml_plr_resampled_nn.coef[0])\n",
    "    \n",
    "\n",
    "# Convert bootstrap estimates to numpy arrays\n",
    "bootstrap_estimates_rf = np.array(bootstrap_estimates_rf)\n",
    "bootstrap_estimates_xgb = np.array(bootstrap_estimates_xgb)\n",
    "bootstrap_estimates_nn = np.array(bootstrap_estimates_nn)\n",
    "\n",
    "# Calculate mean, variance, and confidence intervals for the bootstrap estimates\n",
    "mean_estimate_rf = np.mean(bootstrap_estimates_rf)\n",
    "variance_estimate_rf = np.var(bootstrap_estimates_rf)\n",
    "ci_lower_rf = np.percentile(bootstrap_estimates_rf, 2.5)\n",
    "ci_upper_rf = np.percentile(bootstrap_estimates_rf, 97.5)\n",
    "\n",
    "mean_estimate_xgb = np.mean(bootstrap_estimates_xgb)\n",
    "variance_estimate_xgb = np.var(bootstrap_estimates_xgb)\n",
    "ci_lower_xgb = np.percentile(bootstrap_estimates_xgb, 2.5)\n",
    "ci_upper_xgb = np.percentile(bootstrap_estimates_xgb, 97.5)\n",
    "\n",
    "mean_estimate_nn = np.mean(bootstrap_estimates_nn)\n",
    "variance_estimate_nn = np.var(bootstrap_estimates_nn)\n",
    "ci_lower_nn = np.percentile(bootstrap_estimates_nn, 2.5)\n",
    "ci_upper_nn = np.percentile(bootstrap_estimates_nn, 97.5)\n",
    "\n",
    "print(f'DML with Random Forest Mean Estimate: {mean_estimate_rf}')\n",
    "print(f'DML with Random Forest Variance Estimate: {variance_estimate_rf}')\n",
    "print(f'DML with Random Forest 95% Confidence Interval: [{ci_lower_rf}, {ci_upper_rf}]')\n",
    "\n",
    "print(f'DML with XGBoost Mean Estimate: {mean_estimate_xgb}')\n",
    "print(f'DML with XGBoost Variance Estimate: {variance_estimate_xgb}')\n",
    "print(f'DML with XGBoost 95% Confidence Interval: [{ci_lower_xgb}, {ci_upper_xgb}]')\n",
    "\n",
    "print(f'DML with Neural Network Mean Estimate: {mean_estimate_nn}')\n",
    "print(f'DML with Neural Network Variance Estimate: {variance_estimate_nn}')\n",
    "print(f'DML with Neural Network 95% Confidence Interval: [{ci_lower_nn}, {ci_upper_nn}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------------+--------------------+-------------------+----------------------+------------------------------------------+\n",
      "|   |         Method          |   Mean Estimate    |        ATT        |  Variance Estimate   |         95% Confidence Interval          |\n",
      "+---+-------------------------+--------------------+-------------------+----------------------+------------------------------------------+\n",
      "| 0 |           PSM           | 3.8519239567903627 | 3.794118484040032 |  0.0815964630959717  |  [3.189316577515217, 4.342124608303891]  |\n",
      "| 1 | DML with Random Forest  | 3.8198245438317913 |   [3.84972142]    | 0.027804319027535915 | [3.4807645073912354, 4.112833844403021]  |\n",
      "| 2 |    DML with XGBoost     | 3.3949875019273335 |   [3.50029822]    | 0.07055016623846297  | [2.9395560822871447, 3.9222282668418345] |\n",
      "| 3 | DML with Neural Network | 3.8278507106009094 |   [3.82749805]    | 0.060833183529511524 |  [3.360617363630418, 4.320482391145038]  |\n",
      "+---+-------------------------+--------------------+-------------------+----------------------+------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame({\n",
    "    'Method': ['PSM', 'DML with Random Forest', 'DML with XGBoost', 'DML with Neural Network'],\n",
    "    'Mean Estimate': [mean_estimate_psm, mean_estimate_rf, mean_estimate_xgb, mean_estimate_nn],\n",
    "    'ATT': [att_psm, treatment_effect_rf, treatment_effect_xgb, treatment_effect_nn],\n",
    "    'Variance Estimate': [variance_estimate_psm, variance_estimate_rf, variance_estimate_xgb, variance_estimate_nn],\n",
    "    '95% Confidence Interval': [\n",
    "        f'[{ci_lower_psm}, {ci_upper_psm}]',\n",
    "        f'[{ci_lower_rf}, {ci_upper_rf}]',\n",
    "        f'[{ci_lower_xgb}, {ci_upper_xgb}]',\n",
    "        f'[{ci_lower_nn}, {ci_upper_nn}]'\n",
    "    ]\n",
    "})\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "print(tabulate(results, headers='keys', tablefmt='pretty'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
